"""
íŠ¸ë Œë“œ ìˆ˜ì§‘ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""
from loguru import logger
from datetime import datetime

from .schemas import TrendCollectionResponse

# API Clients
from ..clients.youtube_client import YouTubeClient
from ..clients.rss_client import RSSClient
from ..clients.scraper_client import ScraperClient

# Repositories
from .repositories.keyword_repo import KeywordRepository
from .repositories.youtube_repo import YouTubeRepository
from .repositories.news_repo import NewsRepository

class TrendService:
    """íŠ¸ë Œë“œ ìˆ˜ì§‘ ë° ë¶„ì„ ì„œë¹„ìŠ¤ (Repository Pattern ì ìš©)"""
    
    def __init__(self):
        # Clients
        self.youtube_client = YouTubeClient()
        self.rss_client = RSSClient()
        self.scraper_client = ScraperClient()
        
        # Repositories (Raw SQL ë°©ì‹ì´ë¯€ë¡œ ì„¸ì…˜ ë¶ˆí•„ìš”)
        self.keyword_repo = KeywordRepository()
        self.youtube_repo = YouTubeRepository()
        self.news_repo = NewsRepository()

    async def collect_trending_contents(self, country: str) -> TrendCollectionResponse:
        """
        ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  ìˆ˜ì§‘ ë¡œì§
        """
        logger.info(f"ğŸ”¥ ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  ìˆ˜ì§‘ ì‹œì‘: {country}")
        
        # 1. í‚¤ì›Œë“œ ID í™•ë³´
        keyword_obj = await self.keyword_repo.get_or_create_daily_keyword(country)
        # Raw SQL ê²°ê³¼ëŠ” Dictì´ë¯€ë¡œ ['id'] ì ‘ê·¼
        keyword_id = keyword_obj['id']
        
        # 2. YouTube ìˆ˜ì§‘
        youtube_count = 0
        videos = await self.youtube_client.get_trending_videos(country, max_results=20)
        
        # [Plan B] í•œêµ­ì¸ë° 0ê°œë©´ ì‹¤ê²€ ê¸°ë°˜ ê²€ìƒ‰
        if not videos and country == 'KR':
            logger.warning("âš ï¸ YouTube Trending 0ê°œ -> ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¡œ ëŒ€ì²´ ìˆ˜ì§‘ ì‹œë„")
            signal_keywords = await self.scraper_client.crawl_signal_bz()
            if signal_keywords:
                top_keyword = signal_keywords[0]['keyword']
                logger.info(f"ğŸ” ëŒ€ì²´ ê²€ìƒ‰ì–´: {top_keyword}")
                videos = await self.youtube_client.search_videos(top_keyword, max_results=10)

        # 3. YouTube ì €ì¥
        if videos:
            result = await self.youtube_repo.save_videos(keyword_id, country, videos)
            youtube_count = result['saved'] + result['skipped']
            logger.info(f"âœ… YouTube ì²˜ë¦¬: ì‹ ê·œ {result['saved']}, ì¤‘ë³µ {result['skipped']}")
        
        # 4. News/Signal ìˆ˜ì§‘
        news_count = 0
        articles = await self.rss_client.fetch_google_news(country)
        
        if country == 'KR':
            signal_keywords = await self.scraper_client.crawl_signal_bz()
            if signal_keywords:
                logger.info(f"âœ… Signal.bz ì¶”ê°€: {len(signal_keywords)}ê°œ")
                for item in reversed(signal_keywords): # ì—­ìˆœ insertë¡œ ìˆœì„œ ìœ ì§€
                    articles.insert(0, {
                        'keyword': f"ğŸ”¥ {item['keyword']}",
                        'url': '',
                        'published_at': datetime.now().isoformat()
                    })

        if articles:
            # RSS í¬ë§· -> DB ëª¨ë¸ ìŠ¤í‚¤ë§ˆ ë§¤í•‘
            news_list = []
            for article in articles:
                # URL ìƒì„± ë¡œì§
                final_url = article.get('url')
                if not final_url and 'ğŸ”¥' in article['keyword']:
                     clean_keyword = article['keyword'].replace('ğŸ”¥ ', '')
                     final_url = f"https://www.google.com/search?q={clean_keyword}"

                news_list.append({
                    'title': article['keyword'],
                    'source': 'Google News' if 'keyword' in article and 'ğŸ”¥' not in article['keyword'] else 'ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´',
                    'description': '',
                    'url': final_url or '',
                    'published_at': article.get('published_at') or datetime.now().isoformat()
                })
            
            # 5. News ì €ì¥
            await self.news_repo.save_articles(keyword_id, country, news_list)
            news_count = len(news_list)
            logger.info(f"âœ… News ì €ì¥: {news_count}ê°œ")
        
        # 6. í†µê³„ ì—…ë°ì´íŠ¸ (Commitì€ Repo ë‚´ë¶€ executeì—ì„œ ìˆ˜í–‰ë¨)
        await self.keyword_repo.update_statistics(keyword_id)
        
        total = youtube_count + news_count
        logger.info(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total}ê±´")
        
        return TrendCollectionResponse(
            success=True,
            message=f"ì½˜í…ì¸  {total}ê°œ ìˆ˜ì§‘ ì™„ë£Œ",
            keywords_count=total
        )
