================================================================================
작업 일지 - 2026년 1월 20일
================================================================================

작업 시작 시각: 12:43
작업자: Antigravity AI Agent


오늘의 목표
================================================================================

1. n8n 워크플로우를 Python 기반 시스템으로 전환하기 위한 설계 문서 작성
2. API 수집 전략 수립
3. 점수 산정 알고리즘 설계
4. 프로젝트 구조 및 데이터베이스 스키마 정의


완료한 작업
================================================================================

1. 프로젝트 개요 문서 작성
   파일: Project_Docs/01_프로젝트_개요.md
   내용:
   - 수집, 선별, 발행 3단계 프로세스 정의
   - 기술 스택 선정 (FastAPI, PostgreSQL, Redis, LangChain)
   - 예상 월간 비용 산정 (54~77달러)
   - n8n 대비 Python 개발의 장점 정리

2. API 수집 전략 문서 작성
   파일: Project_Docs/02_API_수집_전략.md
   내용:
   - Google Trends API (Apify) 활용 방법
   - YouTube Data API v3 무료 할당량 관리
   - Instagram Scraper 크레딧 절약 전략
   - Google News RSS 활용
   - Threads API 공식/사설 구분
   - 수집 스케줄 및 에러 핸들링 전략

3. 점수 산정 알고리즘 문서 작성
   파일: Project_Docs/03_점수_산정_알고리즘.md
   내용:
   - 인기도 점수 (40점): 조회수, 참여도, 언급량
   - 상승세 점수 (30점): 증가율, 가속도 기반 급상승 감지
   - 최근성 점수 (20점): 시간 감쇠 함수 적용
   - 다출처 점수 (10점): 플랫폼 다양성 평가
   - 총 100점 만점 체계

4. 프로젝트 구조 문서 작성
   파일: Project_Docs/04_프로젝트_구조.md
   내용:
   - 전체 디렉토리 구조 설계
   - 모듈별 역할 정의 (collectors, processors, publishers)
   - FastAPI 라우터 및 서비스 레이어 분리
   - 비동기 처리 및 스케줄러 설정
   - requirements.txt 의존성 패키지 리스트

5. 데이터베이스 스키마 문서 작성
   파일: Project_Docs/05_데이터베이스_스키마.md
   내용:
   - 8개 테이블 설계 (trending_keywords, raw_data, normalized_data 등)
   - 테이블 간 관계 정의 (ERD)
   - 인덱스 최적화 전략
   - 데이터 보관 정책 (7일~1년)
   - Alembic 마이그레이션 명령어 정리


주요 의사결정 사항
================================================================================

1. 기술 스택
   - 백엔드: FastAPI (비동기 처리 최적화)
   - 데이터베이스: PostgreSQL (JSONB 지원으로 유연한 메타데이터 저장)
   - 캐시: Redis (중복 체크 및 API 응답 캐싱)
   - 스케줄러: APScheduler (Python 네이티브, 간단한 설정)
   - LLM: OpenAI GPT-4 (게시글 자동 생성)

2. API 선택
   - Google Trends: Apify 유료 ($7.99/월)
     이유: 정확도 높은 트렌드 지표, 국가별 세분화
   - YouTube: 공식 API 무료
     이유: 일일 할당량(10,000 units) 충분
   - Instagram: Apify 사설 API
     이유: 공식 API로는 타인 게시물 수집 불가
   - News: Google RSS 무료
     이유: 보조 데이터 소스로 충분

3. 점수 산정 가중치
   - 인기도 40%: 실제 관심도 반영
   - 상승세 30%: 급상승 트렌드 포착 중요
   - 최근성 20%: 신선한 이슈 우선
   - 다출처 10%: 보조 지표

4. 데이터 보관 정책
   - 원시 데이터: 30일 (용량 관리)
   - 정규화 데이터: 60일 (분석 기간)
   - 점수 데이터: 1년 (트렌드 분석)


기술적 도전 과제
================================================================================

1. API 할당량 관리
   문제: YouTube API 일일 10,000 units 제한
   해결: 
   - search.list (100 units) 사용 최소화
   - videos.list (1 unit) 우선 활용
   - 국가별 순차 수집으로 분산

2. 데이터 중복 제거
   문제: 같은 콘텐츠가 여러 소스에서 수집됨
   해결:
   - URL 기준 UNIQUE 제약
   - 제목 유사도 분석 (cosine similarity)
   - Redis로 빠른 중복 체크

3. 실시간성 vs 비용
   문제: 실시간 수집 시 API 비용 급증
   해결:
   - 하루 3회 주요 수집 (09:00, 15:00, 21:00)
   - 무료 API (RSS)는 매시간 수집
   - 캐싱으로 중복 호출 방지

4. 점수 산정 정확도
   문제: 플랫폼마다 지표 의미가 다름 (조회수 vs 좋아요)
   해결:
   - 정규화 (Min-Max Scaling)로 동일 척도 변환
   - 로그 변환으로 극단값 완화
   - A/B 테스트로 가중치 조정


다음 작업 계획
================================================================================

1. 개발 환경 설정
   - 가상환경 생성 (venv)
   - requirements.txt 작성
   - .env 파일 템플릿 생성
   - .gitignore 설정

2. 핵심 모듈 개발 (우선순위 순)
   1) core/config.py - 환경변수 로드
   2) core/database.py - DB 연결 설정
   3) models/ - SQLAlchemy 모델 작성
   4) Alembic 초기화 및 마이그레이션
   5) collectors/google_trends.py - 첫 수집기 구현
   6) processors/scorer.py - 점수 산정 로직
   7) api/keywords.py - REST API 엔드포인트

3. 테스트 및 검증
   - 샘플 데이터로 수집 테스트
   - 점수 알고리즘 검증
   - API 응답 속도 측정

4. 스케줄러 설정
   - APScheduler 작업 등록
   - 크론 표현식 설정
   - 에러 알림 설정


참고 사항
================================================================================

- 모든 문서는 사용자 규칙에 따라 마크다운 문법 미사용
- 구분선(===, ---), 들여쓰기, 번호 매기기로 가독성 확보
- 백엔드 명령어는 가상환경(venv) 내에서 실행 필수
- 모듈 설치는 requirements.txt에 추가 후 사용자가 직접 설치


작업 종료 시각: 진행 중

================================================================================
